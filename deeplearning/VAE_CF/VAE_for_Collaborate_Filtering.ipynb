{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6AHAlMY_dHvz"
   },
   "source": [
    "1) Metric - ndcg, recall\n",
    "\n",
    "2) Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "XzYBIuyH6seC",
    "outputId": "7722a6a4-a801-4dcd-8a44-ac9687288de1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "assert tf.__version__ == '2.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YviAkXCorSVU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import basename, normpath\n",
    "import urllib.request\n",
    "import requests\n",
    "import tarfile\n",
    "import tempfile\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_z4sv48IgW_"
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JO7i0F6xaFJ6"
   },
   "outputs": [],
   "source": [
    "def argsort_sparse(m_sp, R=None):\n",
    "    '''\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m_sp : scipy sparse matrix\n",
    "\n",
    "    R(optional) : int\n",
    "        maximum number of keeping indexs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Sorted Indexs for nonzero data\n",
    "    '''\n",
    "    row_inds, col_inds = m_sp.nonzero()\n",
    "    tuples = zip(row_inds, col_inds, m_sp.data)\n",
    "    sorted_tuples = list(sorted(tuples))\n",
    "\n",
    "    n_rows, n_cols = m_sp.shape\n",
    "\n",
    "    results = []\n",
    "    tup_idx = 0\n",
    "    for r in range(n_rows):\n",
    "        results.append([])\n",
    "        for i, tup in enumerate(sorted_tuples[tup_idx:]):\n",
    "            if tup[0] == r:\n",
    "                results[-1].append(tup[1])\n",
    "            else:\n",
    "                tup_idx += i\n",
    "                break\n",
    "\n",
    "    if R is not None:\n",
    "        results = list(map(lambda l:l[:R], results))\n",
    "\n",
    "\n",
    "\n",
    "    return np.array(results)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def recall_at_r(x_true, x_predicted, R):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_true : scipy sparse matrix or numpy array\n",
    "        true data\n",
    "    x_predicted : numpy array\n",
    "        predicted recommendations\n",
    "    R : int \n",
    "        hyper-parameter for this metric\n",
    "\n",
    "    Returs\n",
    "    ------\n",
    "    Recall@R\n",
    "\n",
    "    '''\n",
    "\n",
    "    if type(x_true) == np.array:\n",
    "        sorted_col_inds = np.argsort(x_true, axis=-1).reshape(-1)\n",
    "    else:\n",
    "        sorted_col_inds = argsort_sparse(x_true, R).reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "    row_inds = np.repeat(np.array(list(range(x_true.shape[0]))), R)\n",
    "\n",
    "\n",
    "    x_true_cp = sp.csr_matrix(([1]*(R*x_true.shape[0]), (list(row_inds), list(sorted_col_inds)) ), shape=x_true.shape)\n",
    "    sorted_idxs_predicted = np.argsort(x_predicted, axis=-1)\n",
    "    selected = np.take_along_axis(x_true_cp, sorted_idxs_predicted[:, :R], axis=-1)\n",
    "    hit = selected.sum(axis=-1)\n",
    "    maxhit = np.minimum(x_true.getnnz(axis=1), R)\n",
    "\n",
    "    return np.squeeze(np.array(hit)) / maxhit\n",
    "\n",
    "def dcg(x_true, x_predicted, R):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "\n",
    "    n_rows, n_cols = x_true.shape\n",
    "\n",
    "    if type(x_true) == np.array:\n",
    "        sorted_col_inds = np.argsort(x_true, axis=-1).reshape(-1)\n",
    "    else:\n",
    "        sorted_col_inds = argsort_sparse(x_true, R).reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "    row_inds = np.repeat(np.array(list(range(x_true.shape[0]))), R)\n",
    "\n",
    "\n",
    "    x_true_cp = sp.csr_matrix(([1]*(R*x_true.shape[0]), (list(row_inds), list(sorted_col_inds)) ), shape=x_true.shape)\n",
    "    sorted_idxs_predicted = np.argsort(x_predicted, axis=-1)\n",
    "\n",
    "    '''\n",
    "    results = np.zeros((n_rows, 1))\n",
    "    for r in range(1, R+1, 1):\n",
    "        selected = np.take_along_axis(x_true_cp, sorted_idxs_predicted[:, r-1:r], axis=-1).toarray()\n",
    "        denominator = np.log(r+1)\n",
    "        nominator = np.power(2, selected) - 1\n",
    "        results += nominator / denominator\n",
    "    '''\n",
    "    \n",
    "    selected = np.take_along_axis(x_true_cp, sorted_idxs_predicted[:,:R], axis=-1).toarray()\n",
    "    denominator = np.expand_dims(np.log2(np.arange(2,R+2,1)), axis=0)\n",
    "    results = selected / denominator\n",
    "    results = results.sum(axis=-1)\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysrLp1NZqH70"
   },
   "outputs": [],
   "source": [
    "DATASETS_DIR = './'\n",
    "# the different datasets\n",
    "ML_20M = 'ml-20m'\n",
    "ML_20M_ALT = 'ml-20m_alt'\n",
    "NETFLIX = 'netflix'\n",
    "LASTFM = 'lastfm'\n",
    "PINTEREST = 'pinterest'\n",
    "DATASETS = [ML_20M, NETFLIX, LASTFM, PINTEREST, ML_20M_ALT]\n",
    "\n",
    "# download urls to different datasets\n",
    "DOWNLOAD_URL = {\n",
    "    ML_20M: 'http://files.grouplens.org/datasets/movielens/ml-20m.zip',\n",
    "    NETFLIX: 'https://archive.org/download/nf_prize_dataset.tar/nf_prize_dataset.tar.gz',\n",
    "    LASTFM: 'http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-360K.tar.gz',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ou4fHg5yqCBR"
   },
   "outputs": [],
   "source": [
    "def download_file(url, filename):\n",
    "    if not os.path.isdir(DATASETS_DIR):\n",
    "        os.makedirs(DATASETS_DIR)\n",
    "\n",
    "    u = urllib.request.urlopen(url)\n",
    "    with open(filename, 'wb') as f:\n",
    "        meta = u.info()\n",
    "        if (meta.get_all(\"Content-Length\")):\n",
    "            file_size = int(meta.get_all(\"Content-Length\")[0])\n",
    "            pbar = tqdm(\n",
    "                total=file_size,\n",
    "                desc=basename(normpath(filename)),\n",
    "                unit='B',\n",
    "                unit_scale=True)\n",
    "\n",
    "            file_size_dl = 0\n",
    "            block_sz = 8192\n",
    "            while True:\n",
    "                buff = u.read(block_sz)\n",
    "                if not buff:\n",
    "                    break\n",
    "                pbar.update(len(buff))\n",
    "                file_size_dl += len(buff)\n",
    "                f.write(buff)\n",
    "            pbar.close()\n",
    "        else:\n",
    "            LOG.warning(\"No content length information\")\n",
    "            file_size_dl = 0\n",
    "            block_sz = 8192\n",
    "            for cyc in itertools.cycle('/â€“\\\\|'):\n",
    "                buff = u.read(block_sz)\n",
    "                if not buff:\n",
    "                    break\n",
    "                print(cyc, end='\\r')\n",
    "                file_size_dl += len(buff)\n",
    "                f.write(buff)\n",
    "\n",
    "\n",
    "\n",
    "def extract_file(path, to_directory):\n",
    "    \"\"\"\n",
    "    Extract file\n",
    "    :param path: Path to compressed file\n",
    "    :param to_directory: Directory that is going to store extracte files\n",
    "    \"\"\"\n",
    "    if (path.endswith(\"tar.gz\")):\n",
    "        tar = tarfile.open(path, \"r:gz\")\n",
    "        tar.extractall(path=to_directory)\n",
    "        tar.close()\n",
    "    elif (path.endswith(\"tar\")):\n",
    "        tar = tarfile.open(path, \"r:\")\n",
    "        tar.extractall(path=to_directory)\n",
    "        tar.close()\n",
    "    elif (path.endswith(\"zip\")):\n",
    "        with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(to_directory)\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Could not extract {} as no appropriate extractor is found\".format(path))\n",
    "\n",
    "def download_movielens():\n",
    "    filepath = os.path.join(DATASETS_DIR, ML_20M_ALT + '.zip')\n",
    "    if not glob(filepath):\n",
    "        download_file(DOWNLOAD_URL[ML_20M], filepath)\n",
    "\n",
    "    #.info(\"Extracting\")\n",
    "    extract_file(filepath, DATASETS_DIR)\n",
    "\n",
    "def download_lastfm():\n",
    "    filepath = os.path.join(DATASETS_DIR, LASTFM + '.tar.gz')\n",
    "    if not glob(filepath):\n",
    "        download_file(DOWNLOAD_URL[LASTFM], filepath)\n",
    "\n",
    "    extract_file(filepath, DATASETS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1MdZRfpCt0Zf",
    "outputId": "871f442d-5ba2-4ed2-c341-a6df1135a185"
   },
   "outputs": [],
   "source": [
    "download_movielens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "colab_type": "code",
    "id": "-QSQnulF4lTF",
    "outputId": "b8ae3b6d-7148-498c-b278-46720e24d61e"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./ml-20m/ratings.csv')\n",
    "df.rating.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OvcNYHP4q8p"
   },
   "outputs": [],
   "source": [
    "df.rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjYUl04Z3SC1"
   },
   "outputs": [],
   "source": [
    "def make_feedback_implicit(feedback, threshold):\n",
    "    return [1 if rating >= threshold else 0 for rating in feedback]\n",
    "\n",
    "def parse_movielens(threshold=4, **kwargs):\n",
    "\n",
    "    source_file = './ml-20m/ratings.csv'\n",
    "    if not glob(source_file):\n",
    "        download_movielens()\n",
    "\n",
    "\n",
    "    df = pd.read_csv(source_file)\n",
    "    df.drop('timestamp', axis=1, inplace=True)\n",
    "    df['rating'].fillna(0.)\n",
    "    df['rating'] = make_feedback_implicit(df['rating'], 3.5)\n",
    "\n",
    "    map_user_id = {u: i for i, u in enumerate(df.userId.unique())}\n",
    "    map_movie_id = {m: i for i, m in enumerate(df.movieId.unique())}\n",
    "\n",
    "    m_sp = sp.csr_matrix(\n",
    "        (df.rating,\n",
    "         ([map_user_id[u] for u in df.userId],\n",
    "          [map_movie_id[m] for m in df.movieId])),\n",
    "        shape=(len(map_user_id), len(map_movie_id))\n",
    "    )\n",
    "\n",
    "    m_sp.eliminate_zeros()\n",
    "\n",
    "    def save_as_npz(m_sp, path):\n",
    "        if not os.path.isdir('./binary'):\n",
    "            os.mkdir('./binary')\n",
    "        sp.save_npz(path, m_sp)\n",
    "    save_as_npz(m_sp, './binary/bin_ml-20m.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4S_nA-mC8s3"
   },
   "outputs": [],
   "source": [
    "def parse_lastfm():\n",
    "    df = pd.read_csv('./lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv', delimiter='\\t', names=['UserId', 'MusicId', 'ArtistName','Plays'])\n",
    "\n",
    "    music_artist_pair = list(zip([str(i) for i in df['MusicId']],[str(i) for i in df['ArtistName']]))\n",
    "\n",
    "    user_id_dict = {id: i for i, id in enumerate(sorted(set(df['UserId'])))}\n",
    "    item_id_dict = {key: i for i, key in enumerate(sorted(set(music_artist_pair)))}\n",
    "\n",
    "    user_idxs = [user_id_dict[user] for user in df['UserId']]\n",
    "    item_idxs = [item_id_dict[item] for item in music_artist_pair]\n",
    "\n",
    "    m_sp = sp.csr_matrix(([1] * df.shape[0], (user_idxs, item_idxs)), shape=(len(user_id_dict), len(item_id_dict)))\n",
    "\n",
    "    def save_as_npz(m_sp, path):\n",
    "        if not os.path.isdir('./binary'):\n",
    "            os.mkdir('./binary')\n",
    "        sp.save_npz(path, m_sp)\n",
    "\n",
    "    save_as_npz(m_sp, './binary/lastfm.npz')\n",
    "    del user_idxs\n",
    "    del item_idxs\n",
    "    del music_artist_pair\n",
    "    del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TC2wt40CIJqC"
   },
   "outputs": [],
   "source": [
    "parse_movielens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLvm2ToA5sfl"
   },
   "outputs": [],
   "source": [
    "data = sp.load_npz('./binary/bin_ml-20m.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FqhzK-6V5yDn"
   },
   "outputs": [],
   "source": [
    "num_users, num_items = data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Whs7AZ_E3KYC",
    "outputId": "eddf71f9-93b7-429c-8e41-d7d18e06d114"
   },
   "outputs": [],
   "source": [
    "num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O4YGa1HsDLcu"
   },
   "outputs": [],
   "source": [
    "def make_generator(sparse_matrix, batch_size):\n",
    "    n, _ = sparse_matrix.shape\n",
    "    buckets = n // batch_size\n",
    "    additional = (n % batch_size != 0)\n",
    "    def generator():\n",
    "        while True:\n",
    "            i = 0\n",
    "            while i < buckets:\n",
    "                batch = sparse_matrix[i*batch_size:(i+1) * batch_size].copy()\n",
    "                batch = batch.tocoo()\n",
    "                idxs = np.stack([batch.row, batch.col], axis=1)\n",
    "                vals = batch.data\n",
    "                yield (idxs, vals)\n",
    "                i += 1\n",
    "\n",
    "            if additional:\n",
    "                batch = sparse_matrix[i*batch_size:]\n",
    "                batch = batch.tocoo()\n",
    "                idxs = np.stack([batch.row, batch.col], axis =1)\n",
    "                vals = batch.data\n",
    "                yield (idxs, vals)\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzAjZnnPvDUB"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q674vXCyDLa3"
   },
   "outputs": [],
   "source": [
    "gen = make_generator(data, BATCH_SIZE)\n",
    "_, movies = data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LNJ3VmnDLYx"
   },
   "outputs": [],
   "source": [
    "dataset_v1 = tf.data.Dataset.from_generator(gen, output_types=(tf.int64, tf.float32)).map(lambda i, v: tf.sparse.SparseTensor(i, v, (BATCH_SIZE, movies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rOm6NgxOJNVM"
   },
   "outputs": [],
   "source": [
    "def make_generator_v2(sparse_matrix, batch_size):\n",
    "    n, _ = sparse_matrix.shape\n",
    "    buckets = n // batch_size\n",
    "    additional = (n % batch_size != 0)\n",
    "    def generator():\n",
    "        while True:\n",
    "            i = 0\n",
    "            while i < buckets:\n",
    "                batch = sparse_matrix[i*batch_size:(i+1) * batch_size].copy()\n",
    "                yield batch.toarray()\n",
    "                i += 1\n",
    "\n",
    "            if additional:\n",
    "                batch = sparse_matrix[i*batch_size:]\n",
    "                yield batch.toarray()\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTO4Lcs5NU24"
   },
   "outputs": [],
   "source": [
    "gen_v2 = make_generator_v2(data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHvw3JVLNLcM"
   },
   "outputs": [],
   "source": [
    "dataset_v2 = tf.data.Dataset.from_generator(gen_v2, output_types=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0lnutBGt6bF"
   },
   "source": [
    "Ratings ==> User * Movies\n",
    "\n",
    "Movie_id ==> Movie Titile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0IabK_KKYpA"
   },
   "outputs": [],
   "source": [
    "class Sparse2Dense(tf.keras.layers.Dense):\n",
    "    def call(self, x):\n",
    "        #print(type(x))\n",
    "        assert type(x) == tf.sparse.SparseTensor\n",
    "        rank = len(x.shape)\n",
    "        if rank != 2:\n",
    "            raise NotImplementedError(\"input rank should be 2\")\n",
    "        else:\n",
    "            outputs = tf.sparse.sparse_dense_matmul(x, self.kernel)\n",
    "        if self.use_bias:\n",
    "            outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)  # pylint: disable=not-callable\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d7j1eAMX60Hn"
   },
   "outputs": [],
   "source": [
    "class VAE_CF(tf.keras.Model):\n",
    "    def __init__(self, items, hidden_dims, latent_dims, *args, **kwargs):\n",
    "        super(VAE_CF, self).__init__(*args, **kwargs)\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "                #tf.keras.layers.Dense(hidden_dims),\n",
    "                Sparse2Dense(hidden_dims),\n",
    "                tf.keras.layers.Dense(hidden_dims),\n",
    "                tf.keras.layers.Dense(2 * latent_dims)\n",
    "        ])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "                #tf.keras.layers.InputLayer(input_shape=(latent_dims)),\n",
    "                tf.keras.layers.Dense(hidden_dims),\n",
    "                tf.keras.layers.Dense(hidden_dims),\n",
    "                tf.keras.layers.Dense(items)\n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, log_var = tf.split(self.encoder(x), 2, 1)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        return mean, log_var, z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        batch = tf.shape(mean)[0]\n",
    "        dim = tf.shape(mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return mean + tf.exp(log_var * .5) * epsilon\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mean, log_var, z = self.encode(inputs)\n",
    "        reconstructed = self.decode(z)\n",
    "        kl_loss = -.5 * tf.math.reduce_mean((log_var - tf.exp(log_var) - tf.square(mean) + 1))\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XllEsnkhr-0b"
   },
   "outputs": [],
   "source": [
    "def encoder_layer(items, hidden_dims, latent_dims, batch_size):\n",
    "    inputs = tf.keras.layers.Input(shape=(items,), sparse=True, batch_size=batch_size)\n",
    "    x = Sparse2Dense(hidden_dims)(inputs)\n",
    "    x = tf.keras.layers.Dense(hidden_dims)(x)\n",
    "    outputs = tf.keras.layers.Dense(2 * latent_dims)(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "def decoder_layer(items, hidden_dims, latent_dims):\n",
    "    inputs = tf.keras.layers.Input(shape=(latent_dims,), batch_size=BATCH_SIZE)\n",
    "    x = tf.keras.layers.Dense(hidden_dims)(inputs)\n",
    "    x = tf.keras.layers.Dense(hidden_dims)(x)\n",
    "    outputs = tf.keras.layers.Dense(items)(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L-Jwii1Wsvyj"
   },
   "outputs": [],
   "source": [
    "class VAE_CF(tf.keras.Model):\n",
    "    def __init__(self, items, hidden_dims, latent_dims, *args, **kwargs):\n",
    "        super(VAE_CF, self).__init__(*args, **kwargs)\n",
    "        self.encoder = encoder_layer(items, hidden_dims, latent_dims, BATCH_SIZE)\n",
    "\n",
    "        self.decoder = decoder_layer(items, hidden_dims, latent_dims)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, log_var = tf.split(self.encoder(x), 2, 1)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        return mean, log_var, z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        batch = tf.shape(mean)[0]\n",
    "        dim = tf.shape(mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return mean + tf.exp(log_var * .5) * epsilon\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mean, log_var, z = self.encode(inputs)\n",
    "        reconstructed = self.decode(z)\n",
    "        kl_loss = -.5 * tf.math.reduce_mean((log_var - tf.exp(log_var) - tf.square(mean) + 1))\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gltqI2RXNsl2"
   },
   "outputs": [],
   "source": [
    "class VAE_CF(tf.keras.Model):\n",
    "    def __init__(self, items, hidden_dims, latent_dims, *args, **kwargs):\n",
    "        super(VAE_CF, self).__init__(*args, **kwargs)\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "                #tf.keras.layers.InputLayer(input_shape=(items,)),\n",
    "                tf.keras.layers.Dense(hidden_dims),\n",
    "                tf.keras.layers.Dense(hidden_dims),\n",
    "                tf.keras.layers.Dense(2 * latent_dims)\n",
    "        ])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "                #tf.keras.layers.InputLayer(input_shape=(latent_dims, )),\n",
    "                tf.keras.layers.Dense(hidden_dims),\n",
    "                tf.keras.layers.Dense(hidden_dims),\n",
    "                tf.keras.layers.Dense(items)\n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, log_var = tf.split(self.encoder(x), 2, 1)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        return mean, log_var, z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        batch = tf.shape(mean)[0]\n",
    "        dim = tf.shape(mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return mean + tf.exp(log_var * .5) * epsilon\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mean, log_var, z = self.encode(inputs)\n",
    "        reconstructed = self.decode(z)\n",
    "        kl_loss = -.5 * tf.math.reduce_mean((log_var - tf.exp(log_var) - tf.square(mean) + 1))\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GqRNxvZmOn8N",
    "outputId": "f414de5c-645f-4c39-81e5-b902c161a600"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "explicit\n",
    "mse\n",
    "poisson\n",
    "\n",
    "implicit\n",
    "weighted mse\n",
    "multinomial\n",
    "sigmoid_cross_entropy\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kFOf6j-POn4j"
   },
   "outputs": [],
   "source": [
    "def get_loss_fn(loss_fn_name='multinomial'):\n",
    "    assert loss_fn_name.lower() in {'mse', 'poisson', 'multinomial', 'sigmoid_ce', 'weighted_mse'}\n",
    "    if loss_fn_name == 'mse':\n",
    "        return tf.keras.losses.MeanSquaredError()\n",
    "    elif loss_fn_name == 'poisson':\n",
    "        raise NotImplementedError(\"poisson\")\n",
    "    elif loss_fn_name == 'multinomial':\n",
    "        return tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    elif loss_fn_name == 'sigmoid_ce':\n",
    "        return lambda label, logit: tf.nn.sigmoid_cross_entropy_with_logits(label, logit)\n",
    "    else:\n",
    "        raise NotImplementedError(\"weighted_mse\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sIsxJF_oJGHs"
   },
   "outputs": [],
   "source": [
    "loss_fn = get_loss_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NaO0ky-1m0qo"
   },
   "outputs": [],
   "source": [
    "vae_cf = VAE_CF(num_items, 400, 200)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = get_loss_fn()\n",
    "loss_metric = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "colab_type": "code",
    "id": "_6L_zNwMxc9-",
    "outputId": "70947e65-0925-4b25-8588-c0d7e1d38783"
   },
   "outputs": [],
   "source": [
    "vae_cf(next(iter(dataset_v1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gfpdXF_yT94t",
    "outputId": "fea42264-3ff8-484f-d5d6-3d8f654be4a7"
   },
   "outputs": [],
   "source": [
    "tf.sparse.SparseTensor([[1,2],[3,4]], [1, 2], [3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygZ38pdG7D_i"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, x, loss_fn, optimizer):\n",
    "    #if type(x) == tf.sparse.SparseTensor:\n",
    "    #    x = tf.sparse.to_dense(x)\n",
    "    with tf.GradientTape() as tape:\n",
    "        logit = model(x)\n",
    "        if type(x) == tf.sparse.SparseTensor:\n",
    "            x = tf.sparse.to_dense(x)\n",
    "        loss = loss_fn(x, logit)\n",
    "        loss += sum(model.losses)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "colab_type": "code",
    "id": "VXnnq5aJ7ED1",
    "outputId": "a841cd86-1c64-47cb-f12c-167133825ad9"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    for step, x in enumerate(dataset_v2):\n",
    "        #print(type(x) == tf.sparse.SparseTensor)\n",
    "        loss = train_step(vae_cf, x, loss_fn, optimizer)\n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 2 == 0:\n",
    "            print(\"step {}, mean loss = {}\".format(step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "colab_type": "code",
    "id": "H2mpUSPMkkCx",
    "outputId": "bc9dff72-74fb-47ca-9ec7-e322cfb28fff"
   },
   "outputs": [],
   "source": [
    "vae_cf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ZHu_b749Rxt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VAE for Collaborate Filtering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
