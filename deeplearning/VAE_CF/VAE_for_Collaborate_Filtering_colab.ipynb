{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE for Collaborate Filtering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AHAlMY_dHvz",
        "colab_type": "text"
      },
      "source": [
        "1) Metric - ndcg, recall\n",
        "\n",
        "2) Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzYBIuyH6seC",
        "colab_type": "code",
        "outputId": "7b409c2a-809c-46af-a0f2-93082574b473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YviAkXCorSVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from os.path import basename, normpath\n",
        "import urllib.request\n",
        "import requests\n",
        "import tarfile\n",
        "import tempfile\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "\n",
        "from glob import glob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_z4sv48IgW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.sparse as sp\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO7i0F6xaFJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def argsort_sparse(m_sp, R=None):\n",
        "    '''\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    m_sp : scipy sparse matrix\n",
        "\n",
        "    R(optional) : int\n",
        "        maximum number of keeping indexs\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Sorted Indexs for nonzero data\n",
        "    '''\n",
        "    row_inds, col_inds = m_sp.nonzero()\n",
        "    tuples = zip(row_inds, col_inds, m_sp.data)\n",
        "    tuples = sorted(tuples, key=lambda tup: tup[1])\n",
        "    tuples = sorted(tuples, key=lambda tup: tup[2], reverse=True)\n",
        "    sorted_tuples = sorted(tuples, key=lambda tup: tup[0])\n",
        "    #sorted_tuples = list(sorted(tuples)) # priority --> data < col_ins < row_inds\n",
        "\n",
        "    n_rows, n_cols = m_sp.shape\n",
        "\n",
        "    results = []\n",
        "    tup_idx = 0\n",
        "    for r in range(n_rows):\n",
        "        results.append([])\n",
        "        for i, tup in enumerate(sorted_tuples[tup_idx:]):\n",
        "            if tup[0] == r:\n",
        "                results[-1].append(tup[1])\n",
        "            else:\n",
        "                tup_idx += i\n",
        "                break\n",
        "        \n",
        "        temp = 0\n",
        "        while len(results[-1]) < R and temp < n_cols:\n",
        "            if not temp in results[-1]:\n",
        "                results[-1].append(temp)\n",
        "\n",
        "            temp += 1\n",
        "\n",
        "    if R is not None:\n",
        "        results = list(map(lambda l:l[:R], results))\n",
        "\n",
        "\n",
        "\n",
        "    return np.array(results)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "def recall_at_r(x_true, x_predicted, R):\n",
        "    '''\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    x_true : scipy sparse matrix or numpy array\n",
        "        true data\n",
        "    x_predicted : numpy array\n",
        "        predicted recommendations\n",
        "    R : int \n",
        "        hyper-parameter for this metric\n",
        "\n",
        "    Returs\n",
        "    ------\n",
        "    Recall@R\n",
        "\n",
        "    '''\n",
        "\n",
        "    if type(x_true) == np.array:\n",
        "        sorted_col_inds = np.argsort(x_true, axis=-1).reshape(-1)\n",
        "    else:\n",
        "        sorted_col_inds = argsort_sparse(x_true, R).reshape(-1)\n",
        "\n",
        "\n",
        "\n",
        "    row_inds = np.repeat(np.array(list(range(x_true.shape[0]))), R)\n",
        "\n",
        "\n",
        "    x_true_cp = sp.csr_matrix(([1]*(R*x_true.shape[0]), (list(row_inds), list(sorted_col_inds)) ), shape=x_true.shape)\n",
        "    sorted_idxs_predicted = np.argsort(x_predicted, axis=-1)\n",
        "    selected = np.take_along_axis(x_true_cp, sorted_idxs_predicted[:, :R], axis=-1)\n",
        "    hit = selected.sum(axis=-1)\n",
        "    maxhit = np.minimum(x_true.getnnz(axis=1), R)\n",
        "\n",
        "    return np.squeeze(np.array(hit)) / maxhit\n",
        "\n",
        "def ndcg(x_true, x_predicted, R):\n",
        "    '''\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x_true : numpy array or scipy sparse matrix\n",
        "      \n",
        "    x_predicted : numpy array or tensorflow tensor\n",
        "\n",
        "    R : int\n",
        "\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Normalized Discounted Cumulative Gain(ndcg)\n",
        "\n",
        "\n",
        "    '''\n",
        "\n",
        "    #TODO : if there is not enough interaction between users and items, or the minimum items that users interacte with is less than R, we need to fix it\n",
        "\n",
        "    n_rows, n_cols = x_true.shape\n",
        "\n",
        "    if type(x_true) == np.array:\n",
        "        sorted_col_inds = np.argsort(x_true, axis=-1).reshape(-1)\n",
        "    else:\n",
        "        sorted_col_inds = argsort_sparse(x_true, R).reshape(-1)\n",
        "\n",
        "\n",
        "\n",
        "    row_inds = np.repeat(np.array(list(range(x_true.shape[0]))), R)\n",
        "\n",
        "    #x_true_cp = if there is interation and within R --> 1. otherwise, 0\n",
        "    x_true_cp = sp.csr_matrix(([1]*(R*x_true.shape[0]), (list(row_inds), list(sorted_col_inds)) ), shape=x_true.shape)\n",
        "    sorted_idxs_predicted = np.argsort(x_predicted, axis=-1)[:, ::-1]\n",
        "    '''\n",
        "    results = np.zeros((n_rows, 1))\n",
        "    for r in range(1, R+1, 1):\n",
        "        selected = np.take_along_axis(x_true_cp, sorted_idxs_predicted[:, r-1:r], axis=-1).toarray()\n",
        "        denominator = np.log(r+1)\n",
        "        nominator = np.power(2, selected) - 1\n",
        "        results += nominator / denominator\n",
        "    '''\n",
        "\n",
        "    selected = np.take_along_axis(x_true_cp, sorted_idxs_predicted[:,:R], axis=-1).toarray() #(batch, R)\n",
        "    #numerator = np.power(2, selected) - 1.\n",
        "    denominator = np.expand_dims(np.log2(np.arange(2,R+2,1)), axis=0)\n",
        "    results = selected / denominator\n",
        "    results = results.sum(axis=-1) #(batch)\n",
        "    #normalizer = np.expand_dims(np.power(2, np.array([1 for _ in range(R)])), axis=0) - 1.\n",
        "    normalizer = np.expand_dims(np.array([1 for _ in range(R)]), axis=0)\n",
        "    normalizer = normalizer / denominator\n",
        "    normalizer = normalizer.sum(axis=-1)\n",
        "\n",
        "    return results / normalizer #(batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opteYNyM1oi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c7fb90f8-4d9f-448b-fc9f-467e201a95ba"
      },
      "source": [
        "sample_true = sp.csr_matrix((list(range(20)), (np.repeat(np.array(list(range(4))), 5),np.tile([1,2,3,4,5], 4))), shape=(4,10))\n",
        "sample_true.toarray()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  1,  2,  3,  4,  0,  0,  0,  0],\n",
              "       [ 0,  5,  6,  7,  8,  9,  0,  0,  0,  0],\n",
              "       [ 0, 10, 11, 12, 13, 14,  0,  0,  0,  0],\n",
              "       [ 0, 15, 16, 17, 18, 19,  0,  0,  0,  0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK9OYD8D3-Lv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "ed2d35dc-aa99-4bbf-a239-860ce740827e"
      },
      "source": [
        "sample_predicted = sp.csr_matrix((list(range(20)), (np.repeat(np.array(list(range(4))), 5),np.tile([1,2,3,4,5], 4))), shape=(4,10))\n",
        "sample_predicted = sample_predicted.toarray()\n",
        "sample_predicted"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  1,  2,  3,  4,  0,  0,  0,  0],\n",
              "       [ 0,  5,  6,  7,  8,  9,  0,  0,  0,  0],\n",
              "       [ 0, 10, 11, 12, 13, 14,  0,  0,  0,  0],\n",
              "       [ 0, 15, 16, 17, 18, 19,  0,  0,  0,  0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1GqsL6J4Dhz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7bccba02-48bc-416f-ab0d-2c32e6cf290a"
      },
      "source": [
        "ndcg(sample_true, sample_predicted, 3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysrLp1NZqH70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASETS_DIR = './'\n",
        "# the different datasets\n",
        "ML_20M = 'ml-20m'\n",
        "ML_20M_ALT = 'ml-20m_alt'\n",
        "NETFLIX = 'netflix'\n",
        "LASTFM = 'lastfm'\n",
        "PINTEREST = 'pinterest'\n",
        "DATASETS = [ML_20M, NETFLIX, LASTFM, PINTEREST, ML_20M_ALT]\n",
        "\n",
        "# download urls to different datasets\n",
        "DOWNLOAD_URL = {\n",
        "    ML_20M: 'http://files.grouplens.org/datasets/movielens/ml-20m.zip',\n",
        "    NETFLIX: 'https://archive.org/download/nf_prize_dataset.tar/nf_prize_dataset.tar.gz',\n",
        "    LASTFM: 'http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-360K.tar.gz',\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou4fHg5yqCBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_file(url, filename):\n",
        "    if not os.path.isdir(DATASETS_DIR):\n",
        "        os.makedirs(DATASETS_DIR)\n",
        "\n",
        "    u = urllib.request.urlopen(url)\n",
        "    with open(filename, 'wb') as f:\n",
        "        meta = u.info()\n",
        "        if (meta.get_all(\"Content-Length\")):\n",
        "            file_size = int(meta.get_all(\"Content-Length\")[0])\n",
        "            pbar = tqdm(\n",
        "                total=file_size,\n",
        "                desc=basename(normpath(filename)),\n",
        "                unit='B',\n",
        "                unit_scale=True)\n",
        "\n",
        "            file_size_dl = 0\n",
        "            block_sz = 8192\n",
        "            while True:\n",
        "                buff = u.read(block_sz)\n",
        "                if not buff:\n",
        "                    break\n",
        "                pbar.update(len(buff))\n",
        "                file_size_dl += len(buff)\n",
        "                f.write(buff)\n",
        "            pbar.close()\n",
        "        else:\n",
        "            LOG.warning(\"No content length information\")\n",
        "            file_size_dl = 0\n",
        "            block_sz = 8192\n",
        "            for cyc in itertools.cycle('/–\\\\|'):\n",
        "                buff = u.read(block_sz)\n",
        "                if not buff:\n",
        "                    break\n",
        "                print(cyc, end='\\r')\n",
        "                file_size_dl += len(buff)\n",
        "                f.write(buff)\n",
        "\n",
        "\n",
        "\n",
        "def extract_file(path, to_directory):\n",
        "    \"\"\"\n",
        "    Extract file\n",
        "    :param path: Path to compressed file\n",
        "    :param to_directory: Directory that is going to store extracte files\n",
        "    \"\"\"\n",
        "    if (path.endswith(\"tar.gz\")):\n",
        "        tar = tarfile.open(path, \"r:gz\")\n",
        "        tar.extractall(path=to_directory)\n",
        "        tar.close()\n",
        "    elif (path.endswith(\"tar\")):\n",
        "        tar = tarfile.open(path, \"r:\")\n",
        "        tar.extractall(path=to_directory)\n",
        "        tar.close()\n",
        "    elif (path.endswith(\"zip\")):\n",
        "        with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(to_directory)\n",
        "    else:\n",
        "        raise Exception(\n",
        "            \"Could not extract {} as no appropriate extractor is found\".format(path))\n",
        "\n",
        "def download_movielens():\n",
        "    filepath = os.path.join(DATASETS_DIR, ML_20M_ALT + '.zip')\n",
        "    if not glob(filepath):\n",
        "        download_file(DOWNLOAD_URL[ML_20M], filepath)\n",
        "\n",
        "    #.info(\"Extracting\")\n",
        "    extract_file(filepath, DATASETS_DIR)\n",
        "\n",
        "def download_lastfm():\n",
        "    filepath = os.path.join(DATASETS_DIR, LASTFM + '.tar.gz')\n",
        "    if not glob(filepath):\n",
        "        download_file(DOWNLOAD_URL[LASTFM], filepath)\n",
        "\n",
        "    extract_file(filepath, DATASETS_DIR)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MdZRfpCt0Zf",
        "colab_type": "code",
        "outputId": "b18855ff-3118-425d-e7c9-0c8e7b0be363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "download_movielens()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ml-20m_alt.zip: 100%|██████████| 199M/199M [00:07<00:00, 26.0MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjYUl04Z3SC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_feedback_implicit(feedback, threshold):\n",
        "    return [1 if rating >= threshold else 0 for rating in feedback]\n",
        "\n",
        "def parse_movielens(threshold=4, **kwargs):\n",
        "\n",
        "    source_file = './ml-20m/ratings.csv'\n",
        "    if not glob(source_file):\n",
        "        download_movielens()\n",
        "\n",
        "\n",
        "    df = pd.read_csv(source_file)\n",
        "    df.drop('timestamp', axis=1, inplace=True)\n",
        "    df['rating'].fillna(0.)\n",
        "    df['rating'] = make_feedback_implicit(df['rating'], 3.5)\n",
        "\n",
        "    map_user_id = {u: i for i, u in enumerate(df.userId.unique())}\n",
        "    map_movie_id = {m: i for i, m in enumerate(df.movieId.unique())}\n",
        "\n",
        "    m_sp = sp.csr_matrix(\n",
        "        (df.rating,\n",
        "         ([map_user_id[u] for u in df.userId],\n",
        "          [map_movie_id[m] for m in df.movieId])),\n",
        "        shape=(len(map_user_id), len(map_movie_id))\n",
        "    )\n",
        "\n",
        "    m_sp.eliminate_zeros()\n",
        "\n",
        "    def save_as_npz(m_sp, path):\n",
        "        if not os.path.isdir('./binary'):\n",
        "            os.mkdir('./binary')\n",
        "        sp.save_npz(path, m_sp)\n",
        "    save_as_npz(m_sp, './binary/bin_ml-20m.npz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4S_nA-mC8s3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_lastfm():\n",
        "    df = pd.read_csv('./lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv', delimiter='\\t', names=['UserId', 'MusicId', 'ArtistName','Plays'])\n",
        "\n",
        "    music_artist_pair = list(zip([str(i) for i in df['MusicId']],[str(i) for i in df['ArtistName']]))\n",
        "\n",
        "    user_id_dict = {id: i for i, id in enumerate(sorted(set(df['UserId'])))}\n",
        "    item_id_dict = {key: i for i, key in enumerate(sorted(set(music_artist_pair)))}\n",
        "\n",
        "    user_idxs = [user_id_dict[user] for user in df['UserId']]\n",
        "    item_idxs = [item_id_dict[item] for item in music_artist_pair]\n",
        "\n",
        "    m_sp = sp.csr_matrix(([1] * df.shape[0], (user_idxs, item_idxs)), shape=(len(user_id_dict), len(item_id_dict)))\n",
        "\n",
        "    def save_as_npz(m_sp, path):\n",
        "        if not os.path.isdir('./binary'):\n",
        "            os.mkdir('./binary')\n",
        "        sp.save_npz(path, m_sp)\n",
        "\n",
        "    save_as_npz(m_sp, './binary/lastfm.npz')\n",
        "    del user_idxs\n",
        "    del item_idxs\n",
        "    del music_artist_pair\n",
        "    del df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC2wt40CIJqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parse_movielens()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLvm2ToA5sfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = sp.load_npz('./binary/bin_ml-20m.npz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqhzK-6V5yDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_users, num_items = data.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whs7AZ_E3KYC",
        "colab_type": "code",
        "outputId": "9e4d4fd1-dd88-49c7-b689-2b0c84b1f998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_users // 500"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "276"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYtnNSuFC0Dv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1769ae16-aca4-4536-e83c-d992c23b7de4"
      },
      "source": [
        "data.getnnz()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12195566"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoZH2pRmGTmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_x, val_x = train_test_split(data, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4YGa1HsDLcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_generator(sparse_matrix, batch_size):\n",
        "    n, _ = sparse_matrix.shape\n",
        "    buckets = n // batch_size\n",
        "    additional = (n % batch_size != 0)\n",
        "    def generator():\n",
        "        while True:\n",
        "            i = 0\n",
        "            while i < buckets:\n",
        "                batch = sparse_matrix[i*batch_size:(i+1) * batch_size].copy()\n",
        "                batch = batch.tocoo()\n",
        "                idxs = np.stack([batch.row, batch.col], axis=1)\n",
        "                vals = batch.data\n",
        "                yield (idxs, vals)\n",
        "                i += 1\n",
        "\n",
        "            if additional:\n",
        "                batch = sparse_matrix[i*batch_size:]\n",
        "                batch = batch.tocoo()\n",
        "                idxs = np.stack([batch.row, batch.col], axis =1)\n",
        "                vals = batch.data\n",
        "                yield (idxs, vals)\n",
        "\n",
        "    return generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzAjZnnPvDUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q674vXCyDLa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen = make_generator(train_x, BATCH_SIZE)\n",
        "_, movies = data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LNJ3VmnDLYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_v1 = tf.data.Dataset.from_generator(gen, output_types=(tf.int64, tf.float32)).map(lambda i, v: tf.sparse.SparseTensor(i, v, (BATCH_SIZE, movies)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOm6NgxOJNVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_generator_v2(sparse_matrix, batch_size):\n",
        "    n, _ = sparse_matrix.shape\n",
        "    buckets = n // batch_size\n",
        "    additional = (n % batch_size != 0)\n",
        "    def generator():\n",
        "        while True:\n",
        "            i = 0\n",
        "            while i < buckets:\n",
        "                batch = sparse_matrix[i*batch_size:(i+1) * batch_size].copy()\n",
        "                yield batch.toarray()\n",
        "                i += 1\n",
        "\n",
        "            if additional:\n",
        "                batch = sparse_matrix[i*batch_size:]\n",
        "                yield batch.toarray()\n",
        "\n",
        "    return generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTO4Lcs5NU24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_v2 = make_generator_v2(train_x, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHvw3JVLNLcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_v2 = tf.data.Dataset.from_generator(gen_v2, output_types=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EZBCSJ_I2VT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_val_generator(sparse_matrix, batch_size):\n",
        "    n, _ = sparse_matrix.shape\n",
        "    buckets = n // batch_size\n",
        "    additional = (n % batch_size != 0)\n",
        "    def generator():\n",
        "        i = 0\n",
        "        while i < buckets:\n",
        "            batch = sparse_matrix[i*batch_size:(i+1) * batch_size].copy()\n",
        "            yield batch.toarray()\n",
        "            i += 1\n",
        "\n",
        "        if additional:\n",
        "            batch = sparse_matrix[i*batch_size:]\n",
        "            yield batch.toarray()\n",
        "\n",
        "    return generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNmRueQ3I7RW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_val = make_val_generator(val_x, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4Ukd073H1fx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset = tf.data.Dataset.from_generator(gen_val, output_types=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0lnutBGt6bF",
        "colab_type": "text"
      },
      "source": [
        "Ratings ==> User * Movies\n",
        "\n",
        "Movie_id ==> Movie Titile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0IabK_KKYpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sparse2Dense(tf.keras.layers.Dense):\n",
        "    def call(self, x):\n",
        "        #print(type(x))\n",
        "        assert type(x) == tf.sparse.SparseTensor\n",
        "        rank = len(x.shape)\n",
        "        if rank != 2:\n",
        "            raise NotImplementedError(\"input rank should be 2\")\n",
        "        else:\n",
        "            outputs = tf.sparse.sparse_dense_matmul(x, self.kernel)\n",
        "        if self.use_bias:\n",
        "            outputs = tf.nn.bias_add(outputs, self.bias)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)  # pylint: disable=not-callable\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7j1eAMX60Hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE_CF_sparse_v1(tf.keras.Model):\n",
        "    def __init__(self, items, hidden_dims, latent_dims, *args, **kwargs):\n",
        "        super(VAE_CF_sparse_v1, self).__init__(*args, **kwargs)\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "                #tf.keras.layers.Dense(hidden_dims),\n",
        "                Sparse2Dense(hidden_dims),\n",
        "                tf.keras.layers.Dense(hidden_dims),\n",
        "                tf.keras.layers.Dense(2 * latent_dims)\n",
        "        ])\n",
        "\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "                #tf.keras.layers.InputLayer(input_shape=(latent_dims)),\n",
        "                tf.keras.layers.Dense(hidden_dims),\n",
        "                tf.keras.layers.Dense(hidden_dims),\n",
        "                tf.keras.layers.Dense(items)\n",
        "        ])\n",
        "\n",
        "    def encode(self, x):\n",
        "        mean, log_var = tf.split(self.encoder(x), 2, 1)\n",
        "        z = self.reparameterize(mean, log_var)\n",
        "        return mean, log_var, z\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def reparameterize(self, mean, log_var):\n",
        "        batch = tf.shape(mean)[0]\n",
        "        dim = tf.shape(mean)[1]\n",
        "        epsilon = tf.random.normal(shape=(batch, dim))\n",
        "        return mean + tf.exp(log_var * .5) * epsilon\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mean, log_var, z = self.encode(inputs)\n",
        "        reconstructed = self.decode(z)\n",
        "        kl_loss = -.5 * tf.math.reduce_mean((log_var - tf.exp(log_var) - tf.square(mean) + 1))\n",
        "        self.add_loss(kl_loss)\n",
        "        return reconstructed\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XllEsnkhr-0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder_layer(items, hidden_dims, latent_dims, batch_size):\n",
        "    inputs = tf.keras.layers.Input(shape=(items,), sparse=True, batch_size=batch_size)\n",
        "    x = Sparse2Dense(hidden_dims)(inputs)\n",
        "    x = tf.keras.layers.Dense(hidden_dims)(x)\n",
        "    outputs = tf.keras.layers.Dense(2 * latent_dims)(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "def decoder_layer(items, hidden_dims, latent_dims):\n",
        "    inputs = tf.keras.layers.Input(shape=(latent_dims,), batch_size=BATCH_SIZE)\n",
        "    x = tf.keras.layers.Dense(hidden_dims)(inputs)\n",
        "    x = tf.keras.layers.Dense(hidden_dims)(x)\n",
        "    outputs = tf.keras.layers.Dense(items)(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-Jwii1Wsvyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE_CF_sparse_v2(tf.keras.Model):\n",
        "    def __init__(self, items, hidden_dims, latent_dims, *args, **kwargs):\n",
        "        super(VAE_CF_sparse_v2, self).__init__(*args, **kwargs)\n",
        "        self.encoder = encoder_layer(items, hidden_dims, latent_dims, BATCH_SIZE)\n",
        "\n",
        "        self.decoder = decoder_layer(items, hidden_dims, latent_dims)\n",
        "\n",
        "    def encode(self, x):\n",
        "        mean, log_var = tf.split(self.encoder(x), 2, 1)\n",
        "        z = self.reparameterize(mean, log_var)\n",
        "        return mean, log_var, z\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def reparameterize(self, mean, log_var):\n",
        "        batch = tf.shape(mean)[0]\n",
        "        dim = tf.shape(mean)[1]\n",
        "        epsilon = tf.random.normal(shape=(batch, dim))\n",
        "        return mean + tf.exp(log_var * .5) * epsilon\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mean, log_var, z = self.encode(inputs)\n",
        "        reconstructed = self.decode(z)\n",
        "        #kl_loss = -.5 * tf.math.reduce_mean((log_var - tf.exp(log_var) - tf.square(mean) + 1))\n",
        "        #print(\"kl loss\", kl_loss)\n",
        "        #self.add_loss(kl_loss)\n",
        "        return reconstructed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gltqI2RXNsl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE_CF_dense_v1(tf.keras.Model):\n",
        "    def __init__(self, items, hidden_dims, latent_dims, *args, anneal_steps_total=20000, anneal_cap=0.2, **kwargs):\n",
        "        super(VAE_CF_dense_v1, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.beta = 0.0\n",
        "        self.anneal_cap = anneal_cap\n",
        "        self.anneal_steps_total = anneal_steps_total\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "                #tf.keras.layers.InputLayer(input_shape=(items,)),\n",
        "                tf.keras.layers.Dense(hidden_dims, name='encoder_dense_1'),\n",
        "                tf.keras.layers.Dense(hidden_dims, name='encoder_dense_2'),\n",
        "                tf.keras.layers.Dense(2 * latent_dims, name='encoder_dense_final')\n",
        "        ])\n",
        "\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "                #tf.keras.layers.InputLayer(input_shape=(latent_dims, )),\n",
        "                tf.keras.layers.Dense(hidden_dims, name='decoder_dense_1'),\n",
        "                tf.keras.layers.Dense(hidden_dims, name='decoder_dense_2'),\n",
        "                tf.keras.layers.Dense(items, name='decoder_dense_final'),\n",
        "                tf.keras.layers.LayerNormalization()\n",
        "        ])\n",
        "\n",
        "    def anneal_step(self, update_step):\n",
        "        self.beta = min(self.anneal_cap, self.anneal_cap * update_step / self.anneal_steps_total)\n",
        "\n",
        "    def encode(self, x, training=True):\n",
        "        inp = self.dropout(x, training)\n",
        "        mean, log_var = tf.split(self.encoder(inp), 2, 1)\n",
        "        z = self.reparameterize(mean, log_var)\n",
        "        return mean, log_var, z\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def reparameterize(self, mean, log_var):\n",
        "        batch = tf.shape(mean)[0]\n",
        "        dim = tf.shape(mean)[1]\n",
        "        epsilon = tf.random.normal(shape=(batch, dim))\n",
        "        return mean + tf.exp(log_var * .5) * epsilon\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        mean, log_var, z = self.encode(inputs, training)\n",
        "        reconstructed = self.decode(z)\n",
        "        kl_loss = -.5 * tf.math.reduce_mean((log_var - tf.exp(log_var) - tf.square(mean) + 1))\n",
        "        #print(self.beta*kl_loss)\n",
        "        self.add_loss(self.beta*kl_loss)\n",
        "        return reconstructed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqRNxvZmOn8N",
        "colab_type": "code",
        "outputId": "be5385bd-c2b7-4e8e-e77e-4cf07a4a4339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''\n",
        "explicit\n",
        "mse\n",
        "poisson\n",
        "\n",
        "implicit\n",
        "weighted mse\n",
        "multinomial\n",
        "sigmoid_cross_entropy\n",
        "'''"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nexplicit\\nmse\\npoisson\\n\\nimplicit\\nweighted mse\\nmultinomial\\nsigmoid_cross_entropy\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFOf6j-POn4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss_fn(loss_fn_name='multinomial'):\n",
        "    assert loss_fn_name.lower() in {'mse', 'poisson', 'multinomial', 'sigmoid_ce', 'weighted_mse'}\n",
        "    if loss_fn_name == 'mse':\n",
        "        return tf.keras.losses.MeanSquaredError()\n",
        "    elif loss_fn_name == 'poisson':\n",
        "        raise NotImplementedError(\"poisson\")\n",
        "    elif loss_fn_name == 'multinomial':\n",
        "        #return tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        def multinomial(labels, logits):\n",
        "            return -tf.math.reduce_mean(tf.math.reduce_sum(tf.nn.log_softmax(logits, axis=1) * labels, axis=-1))\n",
        "        \n",
        "        return multinomial\n",
        "    elif loss_fn_name == 'sigmoid_ce':\n",
        "        return lambda label, logit: tf.nn.sigmoid_cross_entropy_with_logits(label, logit)\n",
        "    else:\n",
        "        raise NotImplementedError(\"weighted_mse\")\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIsxJF_oJGHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = get_loss_fn()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaO0ky-1m0qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae_cf = VAE_CF_dense_v1(num_items, 400, 200)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=True)\n",
        "loss_fn = get_loss_fn()\n",
        "loss_metric = tf.keras.metrics.Mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6L_zNwMxc9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = 0\n",
        "for i in iter(dataset_v2):\n",
        "    if idx < 2:\n",
        "        vae_cf(i)\n",
        "    else:\n",
        "        break\n",
        "    idx += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfpdXF_yT94t",
        "colab_type": "code",
        "outputId": "fe9fff7f-96d9-474d-c78b-2d7a5cd80fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vae_cf.losses"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: id=456, shape=(), dtype=float32, numpy=0.0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygZ38pdG7D_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(model, x, loss_fn, optimizer):\n",
        "    #if type(x) == tf.sparse.SparseTensor:\n",
        "    #    x = tf.sparse.to_dense(x)\n",
        "    with tf.GradientTape() as tape:\n",
        "        logit = model(x, training=True)\n",
        "        if type(x) == tf.sparse.SparseTensor:\n",
        "            x = tf.sparse.to_dense(x)\n",
        "        loss = loss_fn(x, logit)\n",
        "        loss += sum(model.losses)\n",
        "\n",
        "\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ8kBZUyDfJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@tf.function\n",
        "def val_loss(model, val_dataset, loss_fn):\n",
        "    assert isinstance(val_dataset, tf.data.Dataset)\n",
        "\n",
        "    loss_metric = tf.keras.metrics.Mean()\n",
        "    for val_x in val_dataset:\n",
        "        logit = model(val_x, training=False)\n",
        "        if type(x) == tf.sparse.SparseTensor:\n",
        "            val_x = tf.sparse.to_dense(val_x)\n",
        "        loss = loss_fn(val_x, logit)\n",
        "        loss += sum(model.losses)\n",
        "        loss_metric(loss)\n",
        "    return loss_metric.result()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXnnq5aJ7ED1",
        "colab_type": "code",
        "outputId": "2bcc7b56-41fb-4050-8f3b-4ec9c5e4159b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "EPOCHS = 1\n",
        "STEPS_PER_EPOCH = num_users // BATCH_SIZE\n",
        "update_step = 0\n",
        "train_loss_track = []\n",
        "val_loss_track = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    for step, x in enumerate(dataset_v2):\n",
        "        #print(type(x) == tf.sparse.SparseTensor)\n",
        "        vae_cf.anneal_step(update_step)\n",
        "        loss = train_step(vae_cf, x, loss_fn, optimizer)\n",
        "        loss_metric(loss)\n",
        "\n",
        "        update_step += 1\n",
        "\n",
        "        if step % 20 == 0:\n",
        "            print(\"epoch {} at step {} loss = {}\".format(epoch, step, loss))\n",
        "            train_loss_track.append(loss.numpy())\n",
        "            val_loss_track.append(val_loss(vae_cf, val_dataset, loss_fn).numpy())\n",
        "\n",
        "        if step == STEPS_PER_EPOCH:\n",
        "            break "
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 at step 0 loss = 658.8325805664062\n",
            "epoch 0 at step 20 loss = 722.103271484375\n",
            "epoch 0 at step 40 loss = 640.103515625\n",
            "epoch 0 at step 60 loss = 673.4186401367188\n",
            "epoch 0 at step 80 loss = 648.980712890625\n",
            "epoch 0 at step 100 loss = 719.493408203125\n",
            "epoch 0 at step 120 loss = 733.8172607421875\n",
            "epoch 0 at step 140 loss = 676.2261352539062\n",
            "epoch 0 at step 160 loss = 605.3428955078125\n",
            "epoch 0 at step 180 loss = 687.110595703125\n",
            "epoch 0 at step 200 loss = 702.1478271484375\n",
            "epoch 0 at step 220 loss = 655.8302612304688\n",
            "epoch 0 at step 240 loss = 640.4595336914062\n",
            "epoch 0 at step 260 loss = 608.4702758789062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMpTUlPB0NxG",
        "colab_type": "code",
        "outputId": "6e6d16d2-e83a-4d86-9db9-8316b13891df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "val_loss_track"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[699.2807,\n",
              " 698.0057,\n",
              " 696.8747,\n",
              " 4.601901e+25,\n",
              " 83480.55,\n",
              " 2875.6814,\n",
              " 3851.439,\n",
              " 663.2912,\n",
              " 661.61053,\n",
              " inf,\n",
              " 20374.018,\n",
              " 36171.938,\n",
              " 861.29443,\n",
              " 672.587]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cEXr4oZe7d3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "validation\n",
        "held_out user(validation set)\n",
        "held_out item(some itmes that does not participate in reconstruction)\n",
        "\n",
        "0) held_out user should interact with at least K items(filter out) o\n",
        "1) randomly select zero-out items and store indices\n",
        "2) construct new gt interaction matrix at stored indices above one is populated\n",
        "3) input --> zero_out, Dropout off\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}